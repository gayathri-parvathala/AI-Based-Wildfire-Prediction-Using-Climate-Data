# -*- coding: utf-8 -*-
"""final_wf.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QevXMYam3MQERkjvGS1--l7IgT6RxfHr
"""

import numpy as np
import pandas as pd
import xgboost as xgb
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import accuracy_score
from torch.utils.data import DataLoader, TensorDataset
import joblib  # For saving and loading models

# Load dataset
df = pd.read_csv("Fire_dataset_cleaned.csv")

# Drop unnecessary columns
# (You can add any specific columns to drop here if needed, e.g., df.drop(columns=['Unnecessary_Column'], inplace=True))

# Encode "Classes" column (fire=1, no fire=0)
df['Classes'] = df['Classes'].map({'fire': 1, 'not fire': 0})

# Split features and target
X = df.drop(columns=['Classes'])
y = df['Classes']

# Normalize numerical features using RobustScaler (more robust to outliers)
scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)

# Save the scaler
joblib.dump(scaler, 'scaler.pkl')

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# -----------------------------
# ðŸ”¥ Step 1: Feature Selection using XGBoost
# -----------------------------
xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_model.fit(X_train, y_train)

# Get feature importance
feature_importance = xgb_model.feature_importances_
important_features = X.columns[np.argsort(feature_importance)[-10:]]  # Top 10 features

# Use only selected features
X_train = X_train[:, np.argsort(feature_importance)[-10:]]
X_test = X_test[:, np.argsort(feature_importance)[-10:]]

# Convert to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)

# Create DataLoaders
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)  # Increased batch size
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# -----------------------------
# ðŸ”¥ Step 2: Temporal Transformer Model
# -----------------------------
class TemporalTransformer(nn.Module):
    def __init__(self, input_dim, num_classes=2):
        super(TemporalTransformer, self).__init__()
        self.embedding = nn.Linear(input_dim, 64)
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=64, nhead=4), num_layers=2
        )
        self.fc = nn.Linear(64, num_classes)
        self.dropout = nn.Dropout(0.3)  # Added dropout

    def forward(self, x):
        x = self.embedding(x)
        x = x.unsqueeze(1)  # Add sequence dimension
        x = self.transformer(x)
        x = x.mean(dim=1)  # Aggregate over sequence
        x = self.dropout(x)  # Apply dropout
        return self.fc(x)

# Initialize model
model = TemporalTransformer(input_dim=10)  # Using top 10 features from XGBoost
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0005)  # Reduced learning rate

# -----------------------------
# ðŸ”¥ Step 3: Training
# -----------------------------
def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=30, patience=5):
    best_loss = float('inf')
    patience_counter = 0  # Counts epochs without improvement

    model.train()
    for epoch in range(epochs):
        for X_batch, y_batch in train_loader:
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping
            optimizer.step()

        # Validate on test set
        model.eval()
        with torch.no_grad():
            val_loss = sum(criterion(model(X_batch), y_batch).item() for X_batch, y_batch in test_loader) / len(test_loader)

        print(f"Epoch {epoch+1}/{epochs}, Train Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}")

        # Check for early stopping
        if val_loss < best_loss:
            best_loss = val_loss
            patience_counter = 0  # Reset patience
        else:
            patience_counter += 1  # Increase patience counter

        if patience_counter >= patience:
            print("ðŸ›‘ Early stopping triggered!")
            break  # Stop training if no improvement

train_model(model, train_loader, test_loader, criterion, optimizer)


# -----------------------------
# ðŸ”¥ Step 4: Evaluation
# -----------------------------
def evaluate_model(model, test_loader):
    model.eval()
    all_preds, all_labels = [], []
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            outputs = model(X_batch)
            preds = torch.argmax(outputs, dim=1)
            all_preds.extend(preds.numpy())
            all_labels.extend(y_batch.numpy())
    acc = accuracy_score(all_labels, all_preds)
    print(f"ðŸ”¥ Model Accuracy: {acc * 100:.2f}% ðŸ”¥")

evaluate_model(model, test_loader)

# -----------------------------
# ðŸ”¥ Step 5: Save the Trained Model
# -----------------------------
# Save the trained model
torch.save(model.state_dict(), 'temporal_transformer_model.pth')

# Optionally, you can also save the model using joblib if it's an sklearn model
# joblib.dump(xgb_model, 'xgb_model.pkl')  # For saving the XGBoost model if needed

import shap
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
import xgboost as xgb

# Load the dataset again (as X_train is missing in the environment)
df = pd.read_csv("Fire_dataset_cleaned.csv")

# Drop unnecessary columns

# Encode "Classes" column (fire=1, no fire=0)
df['Classes'] = df['Classes'].map({'fire': 1, 'not fire': 0})

# Split features and target
X = df.drop(columns=['Classes'])
y = df['Classes']

# Normalize numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split dataset
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Load the trained XGBoost model (assuming you have already trained it earlier)
xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_model.fit(X_train, y_train)

# Convert the data into a NumPy array for SHAP compatibility
X_train_np = np.array(X_train)

# Step 1: SHAP for XGBoost Model
# Create SHAP explainer
explainer_xgb = shap.Explainer(xgb_model, X_train_np)

# Calculate SHAP values
shap_values_xgb = explainer_xgb(X_train_np)

# Plot SHAP summary plot (Feature importance)
shap.summary_plot(shap_values_xgb, X_train_np)

2.5
import torch
import numpy as np
import joblib  # For loading the scaler
from termcolor import colored  # For colorful output
import pandas as pd  # For using pandas DataFrame
import torch.nn.functional as F  # For applying softmax

# Load the trained model
model = TemporalTransformer(input_dim=10)  # Initialize the model with the same input_dim as during training
model.load_state_dict(torch.load('temporal_transformer_model.pth'))
model.eval()

# Load the scaler used for normalization
scaler = joblib.load('scaler.pkl')

# Define the feature names to match with the trained dataset
feature_names = ['Temperature', 'RH', 'Ws', 'Rain', 'FFMC', 'DMC', 'DC', 'ISI', 'BUI', 'FWI']

# Function to get user input and make predictions
def predict_fire_or_not():
    print("Please enter the following features for prediction:")

    # Get user input
    user_input_dict = {}
    for feature in feature_names:
        user_input_dict[feature] = float(input(f"Enter value for {feature}: "))

    # Create a pandas DataFrame for the user input
    user_input_df = pd.DataFrame([user_input_dict])

    # Normalize the input data using the fitted scaler
    user_input_scaled = scaler.transform(user_input_df)

    # Convert the user input into a torch tensor
    user_input_tensor = torch.tensor(user_input_scaled, dtype=torch.float32)

    # Make a prediction using the trained model
    with torch.no_grad():
        output = model(user_input_tensor)

        # Log the raw output (logits)
        print("Raw model output (logits):", output)

        # Apply Softmax to the raw output to get probabilities
        output_probabilities = F.softmax(output, dim=1)

        # Log the output probabilities
        print("Model output probabilities:", output_probabilities)

        predicted_class = torch.argmax(output_probabilities, dim=1).item()

    # Adding some more visual distinction for the prediction
    print("\n" + "="*50)  # Add a separator line
    if predicted_class == 1:
        print(colored("ðŸ”¥ Prediction: Fire (1) ðŸ”¥", 'red', attrs=['bold', 'underline']))  # Red for fire prediction
    else:
        print(colored("ðŸŒ± Prediction: No Fire (0) ðŸŒ±", 'green', attrs=['bold', 'underline']))  # Green for no fire prediction
    print("="*50)  # Add another separator line

# Call the prediction function
predict_fire_or_not()

